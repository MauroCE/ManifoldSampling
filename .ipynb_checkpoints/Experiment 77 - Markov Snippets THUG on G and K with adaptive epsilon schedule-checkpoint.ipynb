{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6dca38",
   "metadata": {},
   "source": [
    "Similar to Experiment 76 but here the list of $\\epsilon$s is adaptively chosen based on the distance of the particles, as done in Chang's code. The only thing to be careful about is that in Chang's code he was using a uniform kernel, whereas here are using a normal kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b494790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "65995689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand, randn\n",
    "from numpy import ones, exp, log, diag, vstack, pi, array, r_, isfinite, logspace, zeros, eye, quantile\n",
    "from numpy.linalg import norm, solve\n",
    "from numpy.random import default_rng, choice, randint\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.stats import multivariate_normal as MVN\n",
    "from scipy.special import ndtri, ndtr\n",
    "from scipy.stats import uniform as udist\n",
    "from scipy.stats import norm as ndist\n",
    "from scipy.linalg import block_diag\n",
    "\n",
    "import time\n",
    "from math import prod\n",
    "from warnings import catch_warnings, filterwarnings, resetwarnings\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from ipywidgets.widgets import interact\n",
    "\n",
    "\n",
    "from RWM import RWM\n",
    "from Manifolds.Manifold import Manifold\n",
    "from tangential_hug_functions import HugTangentialMultivariate\n",
    "from utils import ESS_univariate, prep_contour\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33159da",
   "metadata": {},
   "source": [
    "# G-and-K Functions and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e80ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GKManifold(Manifold):\n",
    "    def __init__(self, ystar):\n",
    "        self.m = len(ystar)            # Number constraints = dimensionality of the data\n",
    "        self.d = 4                     # Manifold has dimension 4 (like the parameter θ)\n",
    "        self.n = self.d + self.m       # Dimension of ambient space is m + 4\n",
    "        self.ystar = ystar\n",
    "        # N(0, 1) ---> U(0, 10).\n",
    "        self.G    = lambda θ: 10*ndtr(θ)\n",
    "        # U(0, 10) ---> N(0, 1)\n",
    "        self.Ginv = lambda θ: ndtri(θ/10)\n",
    "\n",
    "    def q(self, ξ):\n",
    "        \"\"\"Constraint for G and K.\"\"\"\n",
    "        ξ = r_[self.G(ξ[:4]), ξ[4:]]   # expecting theta part to be N(0, 1)\n",
    "        with catch_warnings():\n",
    "            filterwarnings('error')\n",
    "            try:\n",
    "                return (ξ[0] + ξ[1]*(1 + 0.8*(1 - exp(-ξ[2]*ξ[4:]))/(1 + exp(-ξ[2]*ξ[4:]))) * ((1 + ξ[4:]**2)**ξ[3])*ξ[4:]) - self.ystar\n",
    "            except RuntimeWarning:\n",
    "                raise ValueError(\"Constraint found Overflow warning.\")\n",
    "                \n",
    "    def _q_raw_uniform(self, ξ):\n",
    "        \"\"\"Constraint function expecting ξ[:4] ~ U(0, 10). It doesn't do any warning check.\"\"\"\n",
    "        return (ξ[0] + ξ[1]*(1 + 0.8*(1 - exp(-ξ[2]*ξ[4:]))/(1 + exp(-ξ[2]*ξ[4:]))) * ((1 + ξ[4:]**2)**ξ[3])*ξ[4:]) - self.ystar\n",
    "    def _q_raw_normal(self, ξ):\n",
    "        \"\"\"Same as `_q_raw_uniform` except expects ξ[:4]~N(0,1).\"\"\"\n",
    "        ξ = r_[self.G(ξ[:4]), ξ[4:]] \n",
    "        return self._q_raw_uniform(ξ)\n",
    "\n",
    "    def Q(self, ξ):\n",
    "        \"\"\"Transpose of Jacobian for G and K. \"\"\"\n",
    "        ξ = r_[self.G(ξ[:4]), ξ[4:]]\n",
    "        return vstack((\n",
    "        ones(len(ξ[4:])),\n",
    "        (1 + 0.8 * (1 - exp(-ξ[2] * ξ[4:])) / (1 + exp(-ξ[2] * ξ[4:]))) * ((1 + ξ[4:]**2)**ξ[3]) * ξ[4:],\n",
    "        8 * ξ[1] * (ξ[4:]**2) * ((1 + ξ[4:]**2)**ξ[3]) * exp(ξ[2]*ξ[4:]) / (5 * (1 + exp(ξ[2]*ξ[4:]))**2),\n",
    "        ξ[1]*ξ[4:]*((1+ξ[4:]**2)**ξ[3])*(1 + 9*exp(ξ[2]*ξ[4:]))*log(1 + ξ[4:]**2) / (5*(1 + exp(ξ[2]*ξ[4:]))),\n",
    "        diag(ξ[1]*((1+ξ[4:]**2)**(ξ[3]-1))*(((18*ξ[3] + 9)*(ξ[4:]**2) + 9)*exp(2*ξ[2]*ξ[4:]) + (8*ξ[2]*ξ[4:]**3 + (20*ξ[3] + 10)*ξ[4:]**2 + 8*ξ[2]*ξ[4:] + 10)*exp(ξ[2]*ξ[4:]) + (2*ξ[3] + 1)*ξ[4:]**2 + 1) / (5*(1 + exp(ξ[2]*ξ[4:]))**2))\n",
    "    ))\n",
    "    \n",
    "    def J(self, ξ):\n",
    "        \"\"\"Safely computes Jacobian.\"\"\"\n",
    "        with catch_warnings():\n",
    "            filterwarnings('error')\n",
    "            try:\n",
    "                return self.Q(ξ).T\n",
    "            except RuntimeWarning:\n",
    "                raise ValueError(\"J computation found Runtime warning.\")\n",
    "                \n",
    "    def fullJacobian(self, ξ):\n",
    "        \"\"\"J_f(G(ξ)) * J_G(ξ).\"\"\"\n",
    "        JGbar = block_diag(10*np.diag(ndist.pdf(ξ[:4])), eye(len(ξ[4:])))\n",
    "        return self.J(ξ) @ JGbar\n",
    "                \n",
    "    def log_parameter_prior(self, θ):\n",
    "        \"\"\"IMPORTANT: Typically the prior distribution is a U(0, 10) for all four parameters.\n",
    "        We keep the same prior but since we don't want to work on a constrained space, we \n",
    "        reparametrize the problem to an unconstrained space N(0, 1).\"\"\"\n",
    "        with catch_warnings():\n",
    "            filterwarnings('error')\n",
    "            try:\n",
    "                return udist.logpdf(self.G(θ), loc=0.0, scale=10.0).sum() + ndist.logpdf(θ).sum()\n",
    "            except RuntimeWarning:\n",
    "                return -np.inf\n",
    "            \n",
    "    def logprior(self, ξ):\n",
    "        \"\"\"Computes the prior distribution for G and K problem. Notice this is already reparametrized.\"\"\"\n",
    "        return self.log_parameter_prior(ξ[:4]) - ξ[4:]@ξ[4:]/2\n",
    "\n",
    "    def logη(self, ξ):\n",
    "        \"\"\"log posterior for c-rwm. This is on the manifold.\"\"\"\n",
    "        try:\n",
    "            J = self.J(ξ)\n",
    "            logprior = self.logprior(ξ)\n",
    "            correction_term  = - prod(np.linalg.slogdet(J@J.T))/2 \n",
    "            return  logprior + correction_term\n",
    "        except ValueError as e:\n",
    "            return -np.inf\n",
    "        \n",
    "    def generate_logηϵ(self, ϵ, kernel='normal'):\n",
    "        \"\"\"Returns the log abc posterior for THUG.\"\"\"\n",
    "        if kernel not in ['normal']:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            def log_abc_posterior(ξ):\n",
    "                \"\"\"Log-ABC-posterior.\"\"\"\n",
    "                u = self.q(ξ)\n",
    "                m = len(u)\n",
    "                return self.logprior(ξ) - u@u/(2*ϵ**2) - m*log(ϵ) - m*log(2*pi)/2\n",
    "            return log_abc_posterior\n",
    "            \n",
    "    def logp(self, v):\n",
    "        \"\"\"Log density for normal on the tangent space.\"\"\"\n",
    "        return MVN(mean=zeros(self.d), cov=eye(self.d)).logpdf(v)\n",
    "    \n",
    "    def is_on_manifold(self, ξ, tol=1e-8):\n",
    "        \"\"\"Checks if ξ is on the ystar manifold.\"\"\"\n",
    "        return np.max(abs(self.q(ξ))) < tol\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "OTHER FUNCTIONS\n",
    "\"\"\"    \n",
    "def generate_powers_of_ten(max_exponent, min_exponent):\n",
    "    \"\"\"E.g. generate_powers_of_ten(2, -1) will return 100, 10, 0, 0.1.\"\"\"\n",
    "    number_of_powers = max_exponent + abs(min_exponent) + 1\n",
    "    return logspace(start=max_exponent, stop=min_exponent, num=number_of_powers, endpoint=True)\n",
    "\n",
    "\n",
    "def data_generator(θ0, m, seed):\n",
    "    \"\"\"Stochastic Simulator. Generates y given θ.\"\"\"\n",
    "    rng = default_rng(seed)\n",
    "    z = rng.normal(size=m)\n",
    "    ξ = r_[θ0, z]\n",
    "    return ξ[0] + ξ[1]*(1 + 0.8*(1 - exp(-ξ[2]*ξ[4:]))/(1 + exp(-ξ[2]*ξ[4:]))) * ((1 + ξ[4:]**2)**ξ[3])*ξ[4:]\n",
    "\n",
    "def find_point_on_manifold(ystar, ϵ, max_iter=1000, tol=1.49012e-08):\n",
    "    \"\"\"Find a point on the data manifold.\"\"\"\n",
    "    i = 0\n",
    "    manifold = GKManifold(ystar=ystar)\n",
    "    log_abc_posterior = manifold.generate_logηϵ(ϵ)\n",
    "    with catch_warnings():\n",
    "        filterwarnings('error')\n",
    "        while i <= max_iter:\n",
    "            i += 1\n",
    "            try: \n",
    "                # Sample θ from U(0, 10)\n",
    "                θfixed = randn(4)\n",
    "                function = lambda z: manifold._q_raw_normal(r_[θfixed, z])\n",
    "                z_guess  = randn(manifold.m)\n",
    "                z_found  = fsolve(function, z_guess, xtol=tol)\n",
    "                ξ_found  = r_[θfixed, z_found]\n",
    "                if not isfinite([log_abc_posterior(ξ_found)]):\n",
    "                    pass\n",
    "                else:\n",
    "                    resetwarnings()\n",
    "                    return ξ_found\n",
    "\n",
    "            except RuntimeWarning:\n",
    "                continue\n",
    "        resetwarnings()\n",
    "        raise ValueError(\"Couldn't find a point, try again.\") \n",
    "        \n",
    "        \n",
    "def find_point_on_manifold_from_θ(ystar, θfixed, ϵ, maxiter=2000, tol=1.49012e-08):\n",
    "    \"\"\"Same as the above but we provide the θfixed. Can be used to find a point where\n",
    "    the theta is already θ0.\"\"\"\n",
    "    i = 0\n",
    "    manifold = GKManifold(ystar=ystar)\n",
    "    log_abc_posterior = manifold.generate_logηϵ(ϵ)\n",
    "    function = lambda z: manifold._q_raw_normal(r_[θfixed, z])\n",
    "    with catch_warnings():\n",
    "        filterwarnings('error')\n",
    "        while i <= maxiter:\n",
    "            i += 1\n",
    "            try:\n",
    "                z_guess  = randn(manifold.m)\n",
    "                z_found  = fsolve(function, z_guess, xtol=tol)\n",
    "                ξ_found  = r_[θfixed, z_found]\n",
    "                if not isfinite([log_abc_posterior(ξ_found)]):\n",
    "                    resetwarnings()\n",
    "                    raise ValueError(\"Couldn't find a point.\")\n",
    "                else:\n",
    "                    resetwarnings()\n",
    "                    return ξ_found\n",
    "            except RuntimeWarning:\n",
    "                continue\n",
    "        resetwarnings()\n",
    "        raise ValueError(\"Couldn't find a point, try again.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "572e7bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_setting(m, B, δ, N, quantile_value, ϵ_min, ϵ0, maxiter, thinning=10):\n",
    "    \"\"\"Generates an object from which one can grab the settings. This allows one to run multiple scenarios.\"\"\"\n",
    "    θ0        = array([3.0, 1.0, 2.0, 0.5])      # True parameter value on U(0, 10) scale.\n",
    "    d         = 4 + m                            # Dimensionality of ξ=(θ, z)\n",
    "    ystar     = data_generator(θ0, m, seed=1234) # Observed data\n",
    "    q         = MVN(zeros(d), eye(d))            # Proposal distribution for THUG\n",
    "    ξ0        = find_point_on_manifold_from_θ(ystar=ystar, θfixed=ndtri(θ0/10), ϵ=1e-5, maxiter=5000, tol=1e-15)\n",
    "    resetwarnings()\n",
    "    manifold  = GKManifold(ystar)\n",
    "    return {\n",
    "        'θ0': θ0,\n",
    "        'm' : m,\n",
    "        'd' : d,\n",
    "        'ystar': ystar,\n",
    "        'q': q,\n",
    "        'ξ0': ξ0,\n",
    "        'B': B,\n",
    "        'δ': δ,\n",
    "        'N': N,\n",
    "        'manifold': manifold,\n",
    "        'thinning': thinning,\n",
    "        'quantile_value': quantile_value,\n",
    "        'ϵ_min': ϵ_min,\n",
    "        'ϵ0': ϵ0,\n",
    "        'maxiter': maxiter\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732a214",
   "metadata": {},
   "source": [
    "# Multivariate Markov Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70868941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_project(v, J):\n",
    "    \"\"\"Projects by solving linear system.\"\"\"\n",
    "    return J.T.dot(solve(J.dot(J.T), J.dot(v)))\n",
    "\n",
    "def THUGIntegratorMultivariate(z0, B, δ):\n",
    "    \"\"\"THUG Integrator for the 2D example (ie using gradients, not jacobians).\"\"\"\n",
    "    trajectory = zeros((B + 1, len(z0)))\n",
    "    x0, v0 = z0[:len(z0)//2], z0[len(z0)//2:]\n",
    "    x, v = x0, v0\n",
    "    trajectory[0, :] = z0\n",
    "    # Integrate\n",
    "    for b in range(B):\n",
    "        x = x + δ*v/2\n",
    "        v = v - 2*linear_project(v, SETTINGS50['manifold'].fullJacobian(x))\n",
    "        x = x + δ*v/2\n",
    "        trajectory[b+1, :] = np.hstack((x, v))\n",
    "    return trajectory\n",
    "\n",
    "def generate_THUGIntegratorMultivariate(B, δ):\n",
    "    \"\"\"Returns a THUG integrator for a given B and δ.\"\"\"\n",
    "    integrator = lambda z: THUGIntegratorMultivariate(z, B, δ)\n",
    "    return integrator\n",
    "\n",
    "\n",
    "#### Metropolis-Hastings version for SMC version\n",
    "def THUG_MH(z0, B, δ, logpi):\n",
    "    \"\"\"Similar to THUGIntegratoUnivariateOnlyEnd but this uses a MH step.\"\"\"\n",
    "    x0, v0 = z0[:len(z0)//2], z0[len(z0)//2:]\n",
    "    x, v = x0, v0\n",
    "    logu = np.log(np.random.rand())\n",
    "    for _ in range(B):\n",
    "        x = x + δ*v/2\n",
    "        v = v - 2*linear_project(v, SETTINGS50['manifold'].fullJacobian(x))\n",
    "        x = x + δ*v/2\n",
    "    if logu <= logpi(x) - logpi(x0):\n",
    "        # accept new point\n",
    "        return np.concatenate((x, v))\n",
    "    else:\n",
    "        # accept old point\n",
    "        return z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f8237eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateMarkovSnippetsTHUG:\n",
    "    \n",
    "    def __init__(self, SETTINGS):\n",
    "        \"\"\"Multivariate Markov Snippets SMC samplers corresponding exactly to Algorithm 1 in Christophe's notes.\n",
    "        It uses the Multivariate THUG kernel as its mutation kernel. The sequence of distributions is fixed here \n",
    "        since we provide ϵs, i.e. a list of tolerances which automatically fully specify the posterior \n",
    "        distributions used at each round.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        :param N: Number of particles\n",
    "        :type N:  int\n",
    "        \n",
    "        :param B: Number of bounces for the THUG integrator. Equivalent to `L` Leapfrog steps in HMC.\n",
    "        :type B: int\n",
    "        \n",
    "        :param δ: Step-size used at each bounce, for the THUG integrator.\n",
    "        :type δ: float\n",
    "        \n",
    "        :param d: Dimensionality of the `x` component of each particle, and equally dimensionality of \n",
    "                  `v` component of each particle. Therefore each particle has dimension `2d`.\n",
    "        \n",
    "        :param ϵs: Tolerances that fully specify the sequence of target filamentary distributions.\n",
    "        :type ϵs: iterable\n",
    "        \"\"\"\n",
    "        # Input variables\n",
    "        self.N  = SETTINGS['N']       \n",
    "        self.B  = SETTINGS['B']\n",
    "        self.δ  = SETTINGS['δ']\n",
    "        self.d  = SETTINGS['d']\n",
    "        self.manifold = SETTINGS['manifold']\n",
    "        self.SETTINGS = SETTINGS\n",
    "        self.quantile_value = SETTINGS['quantile_value']\n",
    "        self.ϵ_min = SETTINGS['ϵ_min']\n",
    "        self.ϵs     = []\n",
    "        self.log_ηs = []\n",
    "        self.maxiter = SETTINGS['maxiter']\n",
    "        \n",
    "        # Variables derived from the above\n",
    "        self.ψ = generate_THUGIntegratorMultivariate(self.B, self.δ)\n",
    "    \n",
    "    def initialize_particles(self):\n",
    "        \"\"\"although usually when adaptively choosing the ϵ values one initializes the particles from\n",
    "        the prior, here we initialize the particles from a large ϵ0 instead and sample from it using RMW.\"\"\"\n",
    "        # Initialize first position on the manifold\n",
    "        x0 = self.SETTINGS['ξ0']\n",
    "        # Grab ϵ0\n",
    "        ϵ0 = self.SETTINGS['ϵ0']\n",
    "        log_ηϵ0 = self.manifold.generate_logηϵ(ϵ0)\n",
    "        self.ϵs.append(ϵ0)\n",
    "        self.log_ηs.append(log_ηϵ0)\n",
    "        # Sample using RWM\n",
    "        burn_in = 100\n",
    "        thinning = 10\n",
    "        ### try initializing using Tangential Hug as MCMC kernel?\n",
    "        #TO_BE_THINNED, acceptance = RWM(x0, self.δ, burn_in + thinning*self.N, self.log_ηs[0])\n",
    "        q = MVN(zeros(self.d), eye(self.d))\n",
    "        TO_BE_THINNED, acceptance = HugTangentialMultivariate(x0, self.B*self.δ, self.B, burn_in + thinning*self.N, 0.0, q, log_ηϵ0, self.manifold.fullJacobian, method='linear')\n",
    "        print(\"Initializing particles. Acceptance: \", np.mean(acceptance)*100)\n",
    "        # Thin the samples to obtain the particles\n",
    "        initialized_particles = TO_BE_THINNED[burn_in:][::thinning]\n",
    "        # Refresh velocities and form particles\n",
    "        v0 = np.random.normal(loc=0.0, scale=1.0, size=(self.N, self.d))\n",
    "        z0 = np.hstack((initialized_particles, v0))\n",
    "        self.starting_particles = z0\n",
    "        return z0\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Starts the Markov Snippets sampler.\"\"\"\n",
    "        starting_time = time.time() \n",
    "        N = self.N\n",
    "        B = self.B\n",
    "        \n",
    "        # Initialize particles\n",
    "        z = self.initialize_particles()   # (N, 2d)\n",
    "        \n",
    "        ## Storage\n",
    "        #### Store z_n^{(i)}\n",
    "#         self.ZN  = np.zeros((N, 2*self.d))  #np.zeros((self.P+1, N, 2*self.d))\n",
    "        #### Store z_{n, k}^{(i)} so basically all the N(T+1) particles\n",
    "        self.ZNK  = zeros((1, N*(B+1), 2*self.d)) #np.zeros((self.P, N*(B+1), 2*self.d))\n",
    "        self.Wbar = zeros((1, N*(B+1)))\n",
    "        self.DISTANCES = zeros(N*(B+1))\n",
    "        self.ESS  = [N]\n",
    "        self.K_RESAMPLED = zeros((1, self.N))\n",
    "        \n",
    "        self.ZN = z\n",
    "        # For each target distribution, run the following loop\n",
    "        n = 1 # counts the iteration number, basically equivalent to n before\n",
    "        while n <= self.maxiter:\n",
    "            try:\n",
    "                print(\"Iteration: \", n)\n",
    "                \n",
    "                ##### RECONSTRUCT PATHS\n",
    "                Z = np.apply_along_axis(self.ψ, 1, z)\n",
    "                self.ZNK = np.vstack((self.ZNK, Z.reshape(1, N*(B+1), 2*self.d)))\n",
    "                \n",
    "                ##### ADAPTIVELY CHOOSE ϵ (choose it based on ALL particles, not just the starting points)\n",
    "                ## Compute distances ||f(x)-y|| for all particles\n",
    "                distances = norm(np.apply_along_axis(self.manifold.q, 1, self.ZNK[-1, :, :self.d]), axis=1)\n",
    "                self.DISTANCES = np.vstack((self.DISTANCES, distances))\n",
    "                ## Find ϵ based on quantile value and store it\n",
    "                ϵ = max(self.ϵ_min, quantile(distances, self.quantile_value))\n",
    "                print(\"\\tEpsilon: \", ϵ)\n",
    "                self.ϵs.append(ϵ)\n",
    "                # Use ϵ to construct the target distribution\n",
    "                self.log_ηs.append(self.manifold.generate_logηϵ(ϵ))\n",
    "\n",
    "                ##### COMPUTE WEIGHTS\n",
    "                # Log-Denominator: shared for each point in the same trajectory\n",
    "                log_μnm1_z  = np.apply_along_axis(self.log_ηs[-2], 1, Z[:, 0, :self.d])  # (N, )\n",
    "                log_μnm1_z  = np.repeat(log_μnm1_z, self.B+1, axis=0).reshape(N, B+1)     # (N, B+1)\n",
    "                # Log-Numerator: different for each point on a trajectory.\n",
    "                log_μn_ψk_z = np.apply_along_axis(self.log_ηs[-1], 2, Z[:, :, :self.d])    # (N, B+1)\n",
    "                # Compute weights, normalize and store\n",
    "                W = exp(log_μn_ψk_z - log_μnm1_z)\n",
    "                W = W / W.sum()\n",
    "                self.Wbar = np.vstack((self.Wbar, W.flatten()))\n",
    "\n",
    "                ##### COMPUTE ESS\n",
    "                self.ESS.append(1 / np.sum(W**2))\n",
    "\n",
    "                ##### RESAMPLE PARTICLES\n",
    "                resampling_indeces = choice(a=np.arange(N*(B+1)), size=N, p=W.flatten())\n",
    "                unravelled_indeces = np.unravel_index(resampling_indeces, (N, B+1))\n",
    "                self.K_RESAMPLED = np.vstack((self.K_RESAMPLED, unravelled_indeces[1]))\n",
    "                indeces = np.dstack(unravelled_indeces).squeeze()\n",
    "                z = np.vstack([Z[tuple(ix)] for ix in indeces])     # (N, 2d)\n",
    "\n",
    "                ##### REJUVENATE VELOCITIES\n",
    "                z[:, self.d:] = np.random.normal(loc=0.0, scale=1.0, size=(N, self.d))\n",
    "                self.ZN = np.vstack((self.ZN, z))\n",
    "            except (ValueError, RuntimeError) as e:\n",
    "                print(\"EXITING: \", e)\n",
    "                self.total_time = time.time() - starting_time\n",
    "                return z\n",
    "            n += 1\n",
    "                \n",
    "        self.total_time = time.time() - starting_time\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5d1df09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS50 = generate_setting(m=50, ϵ0=10, B=20, δ=0.01, N=5000, quantile_value=0.01, ϵ_min=0.00001, maxiter=30, thinning=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6aa349ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to make sure all the storing arrays have the desired shapes. \n",
    "# I also need to check that I am computing the distances correctly on ZNK because this array has 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b097dd7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantitate the algorithm\n",
    "MSTHUG = MultivariateMarkovSnippetsTHUG(SETTINGS50)\n",
    "# Sample\n",
    "zP = MSTHUG.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963dd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb48564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c747f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7fb164c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1073edd15ac44d0b9670b180cefbcedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=14, description='n', max=29), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_histogram(n)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_histogram(n):\n",
    "    fig, ax = plt.subplots(figsize=(20, 4))\n",
    "    bins        = np.arange(-0.5, SETTINGS50['B']+0.5, step=1.0)\n",
    "    bins_labels = np.arange(SETTINGS50['B'])\n",
    "    _ = ax.hist(MSTHUG.K_RESAMPLED[n, :], density=True, bins=bins, edgecolor='k', color='lightsalmon')\n",
    "    ax.set_xticks(bins_labels)\n",
    "    ax.set_xticklabels(bins_labels)\n",
    "    plt.show()\n",
    "\n",
    "resetwarnings()\n",
    "interact(plot_histogram, n=(0, len(MSTHUG.ϵs) - 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd5df301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00154466, 0.00169727, 0.00202024, ..., 0.01720993, 0.01691974,\n",
       "        0.01697533],\n",
       "       [0.00154466, 0.00169727, 0.00202024, ..., 0.01720993, 0.01691974,\n",
       "        0.01697533],\n",
       "       [0.01439428, 0.01465161, 0.01091683, ..., 0.01357298, 0.01499456,\n",
       "        0.01312245],\n",
       "       ...,\n",
       "       [0.01587135, 0.01266648, 0.01607642, ..., 0.00699329, 0.01606971,\n",
       "        0.0207762 ],\n",
       "       [0.01062596, 0.01606554, 0.01685299, ..., 0.01283589, 0.01464173,\n",
       "        0.01922337],\n",
       "       [0.01579708, 0.00838818, 0.01977951, ..., 0.01488513, 0.00384265,\n",
       "        0.01239788]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deae5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
