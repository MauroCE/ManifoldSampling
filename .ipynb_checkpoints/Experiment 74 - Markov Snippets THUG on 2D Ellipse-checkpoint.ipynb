{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f7a3b8",
   "metadata": {},
   "source": [
    "# Markov Snippets THUG vs SMC-THUG on 2D Ellipse Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f853ec",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e3cd7",
   "metadata": {},
   "source": [
    "Classic 2D ellipse example, $f(x) = \\log\\mathcal{N}(x\\mid 0, \\Sigma)$ where $\\Sigma = \\text{diag}(0.1, 1)$. We focus on a uniform prior on an ellipse, and use a uniform kernel leading to filamentary distributions of the type\n",
    "$$\n",
    "\\eta_{\\epsilon}(x) \\propto \\mathbb{I}(\\|f(x) - y\\| \\leq \\epsilon)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6626b",
   "metadata": {},
   "source": [
    "### Markov-Snippets THUG (version 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938dca23",
   "metadata": {},
   "source": [
    "Version one corresponds to Algorithm 1 in the manuscript and, given $\\epsilon s = [\\epsilon_0, \\epsilon_1, \\ldots, \\epsilon_P]$, the number of bounces $B$, the number of particles $N$, and the step size $\\delta$, proceeds as follows. Below $\\psi(z)$ corresponds to using the THUG integrator with parameters $B$ and $\\delta$ using the gradient $\\nabla f$ and outputting the trajectory $\\psi(z_0) = [z_0, z_1, \\ldots, z_B]$.\n",
    "\n",
    "- **Initialization**\n",
    "    - Sample $x_0$ on the ellipse $\\mathcal{M}$ up to numerical accuracy using geometric arguments.\n",
    "    - Use RWM with step-size $\\delta$ to sample from $\\eta_{\\epsilon_0}$ starting from $x_0$. Use a burn-in (of $100$ in this case) and thinning (of $10$ in this case). This gives us samples $x_0^{(1:N)}$ from $\\eta_{\\epsilon_0}$.\n",
    "    - Sample velocities $v_0^{(1:N)}\\sim \\mathcal{N}(0, I)$.\n",
    "    - Form initial particles $z_0^{(i)} = (x_0^{(i)}, v_0^{(i)})$ for each $i=1, \\ldots, N$.\n",
    "- **Main Loop**:\n",
    "    - For each iteration $n=1, \\ldots, P$:\n",
    "        - Apply $\\psi$: $Z^{(i)}_{n-1} = \\psi(z_{n-1}^{(i)})$ this has dimension $(N, B + 1, 4)$.\n",
    "        - Compute weights (and normalize them)\n",
    "            $$\n",
    "            \\bar{w}_{n, k}^{(i)} = \\frac{\\eta_{\\epsilon_n}(\\psi^k(z_{n-1}^{(i)}))}{\\eta_{\\epsilon_{n-1}}(z_{n-1}^{(i)})} = \\mathbb{I}(\\|f(x_{n-1, k}^{(i)}) - y\\| \\leq \\epsilon_n)\n",
    "            $$\n",
    "        - Resample particles using (normalized) weights $\\bar{w}_{n, k}$\n",
    "        - Refresh velocities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894fe952",
   "metadata": {},
   "source": [
    "### Markov-Snippets THUG (version 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d74f45",
   "metadata": {},
   "source": [
    "This version is pretty much identical to Version 1. The only difference is that here we use $\\tilde{\\psi}(z)$ which again performs THUG with $B$ bounces with step size $\\delta$, however it only outputs the beginning and end of the trajectory, i.e. $\\tilde{\\psi}(z_0) = [z_0, z_B]$. The initialization is the samem, so we skip to the main loop.\n",
    "\n",
    "- **Main Loop**:\n",
    "    - For each iteration $n=1, \\ldots, P$:\n",
    "        - Apply $\\tilde{\\psi}$: $Z_{n-1}^{(i)} = \\tilde{\\psi}(z_{n-1}^{(i)})$. This has dimension $(N, 2, 4)$.\n",
    "        - Compute weights the same way as version 1.\n",
    "        - Resample particles using the weights.\n",
    "        - Refresh velocities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201012a",
   "metadata": {},
   "source": [
    "### SMC-THUG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006b83a",
   "metadata": {},
   "source": [
    "This is pretty much a standard SMC sampler targeting the sequence of filamentary distributions. The initialization is the same as for the two algorithms above.\n",
    "\n",
    "\n",
    "- **Main Loop**:\n",
    "    - For each iteration $n = 1, \\ldots, P$:\n",
    "        - Refresh Velocities: $v_{n-1}^{(1:N)}\\sim \\mathcal{N}(0, I)$ and re-form particles $z_{n-1}^{(i)} = (x_{n-1}^{(i)}, v_{n-1}^{(i)})$.\n",
    "        - Run THUG sampler (not just the integrator, this has a Metropolis-Hastings step in it) with $B$ bounces and step size $\\delta$ for each of the starting particles and targeting $\\eta_{\\epsilon_{n-1}}$, thus obtaining $z_{n}^{(i)} = \\text{THUG}_{B, \\delta}(z_{n-1}^{(i)})$. Notice that here we do not output the whole trajectory, but we either output the initial point or the final point of the trajectory based on the MH step.\n",
    "        - Compute weights\n",
    "            $$\n",
    "            w_n^{(i)} \\propto \\frac{\\eta_{\\epsilon_n}(x_{n-1}^{(i)})\\varpi(v_{n-1}^{(i)})}{\\eta_{\\epsilon_{n-1}}(x_{n-1}^{(i)})\\varpi(v_{n-1}^{(i)})} \\propto \\frac{\\mathbb{I}(\\|f(x_{n-1}^{(i)}) - y\\| \\leq \\epsilon_n)}{\\mathbb{I}(\\|f(x_{n-1}^{(i)}) - y\\| \\leq \\epsilon_{n-1}} \\propto \\mathbb{I}(\\|f(x_{n-1}^{(i)}) - y\\| \\leq \\epsilon_n)\n",
    "            $$\n",
    "        - Resample particles using weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde7e88",
   "metadata": {},
   "source": [
    "### Old comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2f5a5",
   "metadata": {},
   "source": [
    "- Aim: first attempt at coding the Markov Snippets SMC sampler with THUG mutation kernel, as described by Algorithm 1 in Christophe's notes. \n",
    "- Application: Here we apply it to the ellipse problem.\n",
    "- Important: here we focus on $\\alpha = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21a6d54",
   "metadata": {},
   "source": [
    "A simplified version of Algorithm 1 is basically this:\n",
    "\n",
    "- Initialize particles $z_0^{(1:N)}\\sim \\mu_0$.\n",
    "- For each iteration $n=1, \\ldots, P$: \n",
    "    1. Construct trajectories $z_{n-1, k}^{(1:N)}$ \n",
    "    2. Resample trajectory points down to $N$ particles using weights\n",
    "    $$\n",
    "    \\bar{w}_{n, k} = \\frac{\\mu_n(z_{n-1, k}^{(i)})}{\\mu_n(z_{n-1}^{(i)})}\n",
    "    $$\n",
    "    3. Rejuvenate velocities for the $N$ particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "0002e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import zeros, eye, array, diag, exp\n",
    "from numpy.linalg import solve, norm\n",
    "from numpy.random import choice\n",
    "from scipy.stats import multivariate_normal as MVN\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "from Manifolds.GeneralizedEllipse import GeneralizedEllipse\n",
    "from utils import prep_contour\n",
    "from RWM import RWM\n",
    "from tangential_hug_functions import HugTangential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447db82f",
   "metadata": {},
   "source": [
    "#### Settings for the Ellipse and Filamentary Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "e5d1ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "μ  = zeros(2)\n",
    "Σ  = diag(array([0.1, 1]))\n",
    "level_set_value = -2.9513586307684885\n",
    "ellipse = GeneralizedEllipse(μ, Σ, exp(level_set_value))\n",
    "πellipse = MVN(μ, Σ)\n",
    "f = πellipse.logpdf\n",
    "grad_f = lambda ξ: -solve(Σ, ξ - μ)\n",
    "\n",
    "def generate_ηϵ(ϵ):\n",
    "    \"\"\"Generates ηϵ with a uniform kernel.\"\"\"\n",
    "    def ηϵ(x):\n",
    "        if np.linalg.norm(f(x) - level_set_value) <= ϵ:\n",
    "            return 1.0\n",
    "        else: \n",
    "            return 0.0\n",
    "    return ηϵ\n",
    "\n",
    "def generate_logηϵ(ϵ):\n",
    "    \"\"\"As above, this is for the uniform kernel but this computes the log density.\"\"\"\n",
    "    def logηϵ(ξ):\n",
    "        with np.errstate(divide='ignore'):\n",
    "            return np.log(float(norm(f(ξ) - level_set_value) <= ϵ) / ϵ)\n",
    "    return logηϵ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "4d7bacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bounding_box_for_ellipse(n):\n",
    "    \"\"\"Computes lots of samples of the ellipse to try and find a bounding box for it.\"\"\"\n",
    "    overall_minimum = 0.0\n",
    "    overall_maximum = 0.0\n",
    "    \n",
    "    for i in range(n):\n",
    "        point = ellipse.sample(advanced=True)\n",
    "        minimum, maximum = np.min(point), np.max(point)\n",
    "        if minimum < overall_minimum:\n",
    "            overall_minimum = minimum\n",
    "        if maximum > overall_maximum:\n",
    "            overall_maximum = maximum\n",
    "    \n",
    "    return overall_minimum, overall_maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0193b9",
   "metadata": {},
   "source": [
    "#### Settings for the THUG kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "050dd11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 20\n",
    "δ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "77413d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def THUGIntegratorUnivariate(z0, B, δ, grad):\n",
    "    \"\"\"THUG Integrator for the 2D example (ie using gradients, not jacobians).\"\"\"\n",
    "    trajectory = zeros((B + 1, len(z0)))\n",
    "    x0, v0 = z0[:len(z0)//2], z0[len(z0)//2:]\n",
    "    x, v = x0, v0\n",
    "    trajectory[0, :] = z0\n",
    "    # Integrate\n",
    "    for b in range(B):\n",
    "        x = x + δ*v/2\n",
    "        g = grad(x)\n",
    "        ghat = g / norm(g)\n",
    "        v = v - 2*ghat*(ghat@v)\n",
    "        x = x + δ*v/2\n",
    "        trajectory[b+1, :] = np.hstack((x, v))\n",
    "    return trajectory\n",
    "\n",
    "def generate_THUGIntegratorUnivariate(B, δ):\n",
    "    \"\"\"Returns a THUG integrator for a given B and δ.\"\"\"\n",
    "    grad = lambda ξ: -solve(Σ, ξ - μ) #ellipse.Q(ξ).T.flatten()\n",
    "    integrator = lambda z: THUGIntegratorUnivariate(z, B, δ, grad)\n",
    "    return integrator\n",
    "\n",
    "###############################\n",
    "#### this is for \\tilde{ψ}.\n",
    "###############################\n",
    "def THUGIntegratorUnivariateOnlyEnd(z0, B, δ, grad):\n",
    "    \"\"\"Similar to THUG integrator but one step does B bounces.\"\"\"\n",
    "    trajectory = zeros((2, len(z0)))\n",
    "    x0, v0 = z0[:len(z0)//2], z0[len(z0)//2:]\n",
    "    x, v = x0, v0\n",
    "    trajectory[0, :] = z0\n",
    "    # Integrate\n",
    "    for _ in range(B):\n",
    "        x = x + δ*v/2\n",
    "        g = grad(x)\n",
    "        ghat = g / norm(g)\n",
    "        v = v - 2*ghat*(ghat@v)\n",
    "        x = x + δ*v/2\n",
    "    trajectory[1, :] = np.hstack((x, v))\n",
    "    return trajectory\n",
    "\n",
    "def generate_THUGIntegratorUnivariateOnlyEnd(B, δ):\n",
    "    \"\"\"Returns a THUG integratorOnlyEnd for a given B and δ.\"\"\"\n",
    "    grad = lambda ξ: -solve(Σ, ξ - μ) #ellipse.Q(ξ).T.flatten()\n",
    "    integrator = lambda z: THUGIntegratorUnivariateOnlyEnd(z, B, δ, grad)\n",
    "    return integrator\n",
    "\n",
    "\n",
    "#### Metropolis-Hastings version for SMC version\n",
    "def THUG_MH(z0, B, δ, logpi):\n",
    "    \"\"\"Similar to THUGIntegratoUnivariateOnlyEnd but this uses a MH step.\"\"\"\n",
    "    grad = lambda ξ: -solve(Σ, ξ - μ)\n",
    "    x0, v0 = z0[:len(z0)//2], z0[len(z0)//2:]\n",
    "    x, v = x0, v0\n",
    "    logu = np.log(np.random.rand())\n",
    "    for _ in range(B):\n",
    "        x = x + δ*v/2\n",
    "        g = grad(x)\n",
    "        ghat = g / norm(g)\n",
    "        v = v - 2*ghat*(ghat@v)\n",
    "        x = x + δ*v/2\n",
    "    if logu <= logpi(x) - logpi(x0):\n",
    "        # accept new point\n",
    "        return np.concatenate((x, v))\n",
    "    else:\n",
    "        # accept old point\n",
    "        return z0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d566f",
   "metadata": {},
   "source": [
    "# Markov Snippets THUG (version 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "f9898330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovSnippetsTHUG:\n",
    "    \n",
    "    def __init__(self, N, B, δ, d, ϵs, onlyend=False):\n",
    "        \"\"\"Markov Snippets SMC samplers corresponding exactly to Algorithm 1 in Christophe's notes.\n",
    "        It uses the THUG kernel as its mutation kernel. The sequence of distributions is fixed here \n",
    "        since we provide ϵs, i.e. a list of tolerances which automatically fully specify the posterior \n",
    "        distributions used at each round.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        :param N: Number of particles\n",
    "        :type N:  int\n",
    "        \n",
    "        :param B: Number of bounces for the THUG integrator. Equivalent to `L` Leapfrog steps in HMC.\n",
    "        :type B: int\n",
    "        \n",
    "        :param δ: Step-size used at each bounce, for the THUG integrator.\n",
    "        :type δ: float\n",
    "        \n",
    "        :param d: Dimensionality of the `x` component of each particle, and equally dimensionality of \n",
    "                  `v` component of each particle. Therefore each particle has dimension `2d`.\n",
    "        \n",
    "        :param ϵs: Tolerances that fully specify the sequence of target filamentary distributions.\n",
    "        :type ϵs: iterable\n",
    "        \n",
    "        :param onlyend: If False, we just run MS-SMC with the THUG integrator as kernel. If True, the THUG \n",
    "                        integrator is pretty much the same, but it only outputs the initial and final point, \n",
    "                        as part of the trajectory. Inside the integrator, it still performs B bounces, but \n",
    "                        basically we are changing \\psi to \\tilde{\\psi} which performs B steps at once, and we \n",
    "                        run it only once.\n",
    "        :type onlyend: bool\n",
    "        \"\"\"\n",
    "        # Input variables\n",
    "        self.N  = N       \n",
    "        self.B  = B\n",
    "        self.δ  = δ\n",
    "        self.d  = d\n",
    "        self.ϵs = ϵs       \n",
    "        \n",
    "        # Variables derived from the above\n",
    "        self.P  = len(ϵs) - 1                                       # Number of target distributions\n",
    "        self.ϖ  = MVN(zeros(d), eye(d))                             # Distribution of the velocities\n",
    "        self.ηs = [generate_ηϵ(ϵ) for ϵ in ϵs]                      # List of filamentary distributions \n",
    "#         self.μs = [lambda z: ηϵ(z[:self.d]) for ηϵ in self.ηs]\n",
    "        if not onlyend:\n",
    "            self.ψ = generate_THUGIntegratorUnivariate(B, δ)\n",
    "        else:\n",
    "            self.ψ = generate_THUGIntegratorUnivariateOnlyEnd(B, δ)\n",
    "            self.B = 1\n",
    "    \n",
    "    def initialize_particles(self):\n",
    "        \"\"\"To initialize particles, we sample from a uniform on a large rectangle.\n",
    "        A rectangle of size [-100, 100] should be plenty large.\"\"\"\n",
    "        # Initialize first position on the manifold\n",
    "        x0 = ellipse.sample(advanced=True)\n",
    "        # Generate log-density for self.η[0]\n",
    "        logηϵ0 = generate_logηϵ(self.ϵs[0])\n",
    "        # Sample using RWM\n",
    "        burn_in = 100\n",
    "        TO_BE_THINNED, _ = RWM(x0, self.δ, burn_in + 10*self.N, logηϵ0)\n",
    "        # Thin the samples to obtain the particles\n",
    "        initialized_particles = TO_BE_THINNED[burn_in:][::10]\n",
    "        # Refresh velocities and form particles\n",
    "        v0 = np.random.normal(loc=0.0, scale=1.0, size=(self.N, self.d))\n",
    "        z0 = np.hstack((initialized_particles, v0))\n",
    "        self.starting_particles = z0\n",
    "        return z0\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Starts the Markov Snippets sampler.\"\"\"\n",
    "        starting_time = time.time() \n",
    "        N = self.N\n",
    "        B = self.B\n",
    "        ## Storage\n",
    "        #### Store z_n^{(i)}\n",
    "        self.ZN  = np.zeros((self.P+1, N, 2*self.d))\n",
    "        #### Store z_{n, k}^{(i)} so basically all the N(T+1) particles\n",
    "        self.ZNK  = np.zeros((self.P, N*(B+1), 2*self.d))\n",
    "        self.Wbar = np.zeros((self.P, N*(B+1)))\n",
    "        self.ESS  = np.zeros((self.P))\n",
    "        # Initialize particles\n",
    "        z = self.initialize_particles()   # (N, 2d)\n",
    "        self.ZN[0] = z\n",
    "        # For each target distribution, run the following loop\n",
    "        for n in range(1, self.P+1):\n",
    "            # Compute trajectories\n",
    "            Z = np.apply_along_axis(self.ψ, 1, z) # should have shape (N, B+1, 2d)\n",
    "            self.ZNK[n-1] = Z.reshape(N*(B+1), 2*self.d)\n",
    "            # Compute weights.\n",
    "            #### Denominator: shared for each point in the same trajectory\n",
    "            μnm1_z  = np.apply_along_axis(self.ηs[n-1], 1, Z[:, 0, :self.d])              # (N, )\n",
    "            μnm1_z  = np.repeat(μnm1_z, self.B+1, axis=0).reshape(N, B+1) # (N, B+1)\n",
    "            #### Numerator: different for each point on a trajectory.\n",
    "            μn_ψk_z = np.apply_along_axis(self.ηs[n], 2, Z[:, :, :self.d])                         # (N, B+1)\n",
    "            #### Put weights together\n",
    "            W = μn_ψk_z / μnm1_z #np.exp(log_μn_ψk_z - log_μnm1_z)\n",
    "            #### Normalize weights\n",
    "            W = W / W.sum()\n",
    "            # store weights (remember these are \\bar{w})\n",
    "            self.Wbar[n-1] = W.flatten()\n",
    "            # compute ESS\n",
    "            self.ESS[n-1] = 1 / np.sum(W**2)\n",
    "            # Resample down to N particles\n",
    "            resampling_indeces = choice(a=np.arange(N*(B+1)), size=N, p=W.flatten())\n",
    "            indeces = np.dstack(np.unravel_index(resampling_indeces, (N, B+1))).squeeze()\n",
    "            z = np.vstack([Z[tuple(ix)] for ix in indeces])     # (N, 2d)\n",
    "            \n",
    "            # Rejuvenate velocities of N particles\n",
    "            z[:, self.d:] = np.random.normal(loc=0.0, scale=1.0, size=(N, self.d))\n",
    "            self.ZN[n] = z\n",
    "        self.total_time = time.time() - starting_time\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "af9f6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, doesn't really matter which ϵs we use.\n",
    "ϵs = np.linspace(start=10, stop=0.001, num=20)\n",
    "B = 50\n",
    "δ = 0.1\n",
    "N  = 5000\n",
    "# Instantitate the algorithm\n",
    "MSTHUG = MarkovSnippetsTHUG(N=N, B=B, δ=δ, d=2, ϵs=ϵs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf04216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "zP = MSTHUG.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1741f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "for i in range(4):\n",
    "    ax[i].contour(*prep_contour([-2.5, 2.5], [-2.5, 2.5], 0.01, f), levels=[level_set_value])\n",
    "    ax[i].contour(*prep_contour([-2.5, 2.5], [-2.5, 2.5], 0.01, f), levels=[level_set_value-MSTHUG.ϵs[i], level_set_value+MSTHUG.ϵs[i]], colors='gray')\n",
    "    ax[i].scatter(*MSTHUG.ZN[i, :, :2].T)\n",
    "    ax[i].set_xlim([-5, 5])\n",
    "    ax[i].set_ylim([-5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74580bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_arctan = lambda point: math.atan2(*point[::-1])\n",
    "rc('font',**{'family':'STIXGeneral'})\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "for i in range(4):\n",
    "    _ = ax[i].hist(np.apply_along_axis(compute_arctan, 1, MSTHUG.ZN[i, :, :2]), bins=30, density=True)\n",
    "    ax[i].set_xticks([-math.pi, 0, math.pi])\n",
    "    ax[i].set_xticklabels([r'$-\\mathregular{\\pi}$', r'$0$', r'$\\mathregular{\\pi}$'], fontsize=15)\n",
    "    ax[i].set_title(r'$\\mathregular{\\epsilon}=$' + '{}'.format(ϵs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cd2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This uses all N(T+1) particles and their w̄. \n",
    "fig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "for i in range(3):\n",
    "    _ = ax[i].hist(np.apply_along_axis(compute_arctan, 1, MSTHUG.ZNK[i, :, :2]), bins=30, density=True, weights=MSTHUG.Wbar[i])\n",
    "    ax[i].set_xticks([-math.pi, 0, math.pi])\n",
    "    ax[i].set_xticklabels([r'$-\\mathregular{\\pi}$', r'$0$', r'$\\mathregular{\\pi}$'], fontsize=15)\n",
    "    ax[i].set_title(r'$\\mathregular{\\epsilon}=$' + '{}'.format(ϵs[i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2c35e",
   "metadata": {},
   "source": [
    "# Markov Snippets THUG (version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The aim now is to only output the end of the trajectory with ψ.\n",
    "MSTHUG_ONLYEND = MarkovSnippetsTHUG(N=N, B=B, δ=δ, d=2, ϵs=ϵs, onlyend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "zP_onlyend = MSTHUG_ONLYEND.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc56450",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "for i in range(4):\n",
    "    ax[i].contour(*prep_contour([-2.5, 2.5], [-2.5, 2.5], 0.01, f), levels=[level_set_value])\n",
    "    ax[i].contour(*prep_contour([-2.5, 2.5], [-2.5, 2.5], 0.01, f), levels=[level_set_value-MSTHUG_ONLYEND.ϵs[i], level_set_value+MSTHUG_ONLYEND.ϵs[i]], colors='gray')\n",
    "    ax[i].scatter(*MSTHUG_ONLYEND.ZN[i, :, :2].T)\n",
    "    ax[i].set_xlim([-5, 5])\n",
    "    ax[i].set_ylim([-5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c42e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "for i in range(4):\n",
    "    _ = ax[i].hist(np.apply_along_axis(compute_arctan, 1, MSTHUG_ONLYEND.ZN[i, :, :2]), bins=30, density=True)\n",
    "    ax[i].set_xticks([-math.pi, 0, math.pi])\n",
    "    ax[i].set_xticklabels([r'$-\\mathregular{\\pi}$', r'$0$', r'$\\mathregular{\\pi}$'], fontsize=15)\n",
    "    ax[i].set_title(r'$\\mathregular{\\epsilon}=$' + '{}'.format(ϵs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "for i in range(3):\n",
    "    _ = ax[i].hist(np.apply_along_axis(compute_arctan, 1, MSTHUG_ONLYEND.ZNK[i, :, :2]), bins=30, density=True, weights=MSTHUG_ONLYEND.Wbar[i])\n",
    "    ax[i].set_xticks([-math.pi, 0, math.pi])\n",
    "    ax[i].set_xticklabels([r'$-\\mathregular{\\pi}$', r'$0$', r'$\\mathregular{\\pi}$'], fontsize=15)\n",
    "    ax[i].set_title(r'$\\mathregular{\\epsilon}=$' + '{}'.format(ϵs[i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98e09a",
   "metadata": {},
   "source": [
    "# SMC-THUG (or metropolised version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5cfa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovSnippetsTHUGMetropolised:\n",
    "    \n",
    "    def __init__(self, N, B, δ, d, ϵs):\n",
    "        \"\"\"Metropolised version: for each particle compute the endpoint of trajectory and its weight.\n",
    "        If the weight is positive, we accept the final point, otherwise we accept the initial point. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        :param N: Number of particles\n",
    "        :type N:  int\n",
    "        \n",
    "        :param B: Number of bounces for the THUG integrator. Equivalent to `L` Leapfrog steps in HMC.\n",
    "        :type B: int\n",
    "        \n",
    "        :param δ: Step-size used at each bounce, for the THUG integrator.\n",
    "        :type δ: float\n",
    "        \n",
    "        :param d: Dimensionality of the `x` component of each particle, and equally dimensionality of \n",
    "                  `v` component of each particle. Therefore each particle has dimension `2d`.\n",
    "        \n",
    "        :param ϵs: Tolerances that fully specify the sequence of target filamentary distributions.\n",
    "        :type ϵs: iterable\n",
    "        \"\"\"\n",
    "        # Input variables\n",
    "        self.N  = N       \n",
    "        self.δ  = δ\n",
    "        self.d  = d\n",
    "        self.ϵs = ϵs       \n",
    "        \n",
    "        # Variables derived from the above\n",
    "        self.P  = len(ϵs) - 1                                       # Number of target distributions\n",
    "        self.ϖ  = MVN(zeros(d), eye(d))                             # Distribution of the velocities\n",
    "        self.ηs = [generate_ηϵ(ϵ) for ϵ in ϵs]                      # List of filamentary distributions \n",
    "        \n",
    "        self.B = B\n",
    "    \n",
    "    def initialize_particles(self):\n",
    "        \"\"\"To initialize particles, we sample from a uniform on a large rectangle.\n",
    "        A rectangle of size [-100, 100] should be plenty large.\"\"\"\n",
    "        # Initialize particles by sampling from η_ϵ0 for a large ϵ0 which can be given as an argument.\n",
    "        # Sample a point from the prior\n",
    "        x0 = ellipse.sample(advanced=True) #np.random.uniform(low=-5, high=5, size=(self.d))\n",
    "        # Use RWM starting from x0\n",
    "        logηϵ0 = generate_logηϵ(self.ϵs[0])\n",
    "        burn_in = 100\n",
    "        TO_BE_THINNED, _ = RWM(x0, self.δ, burn_in + 10*self.N, logηϵ0)\n",
    "        # Thin the samples to obtain the particles\n",
    "        initialized_particles = TO_BE_THINNED[burn_in:][::10]\n",
    "        v0 = np.random.normal(loc=0.0, scale=1.0, size=(self.N, self.d))\n",
    "        z0 = np.hstack((initialized_particles, v0))\n",
    "        self.starting_particles = z0\n",
    "        return z0\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Starts the Markov Snippets sampler.\"\"\"\n",
    "        starting_time = time.time()\n",
    "        # Initialize particles\n",
    "        z = self.initialize_particles()   # (N, 2d)\n",
    "        # Storage\n",
    "        self.PARTICLES    = zeros((self.P+1, self.N, 2*self.d))\n",
    "        self.PARTICLES[0] = z\n",
    "        self.WEIGHTS      = zeros((self.P+1, self.N))\n",
    "        self.WEIGHTS[0]   = 1 / self.N\n",
    "        # For each target distribution, run the following loop\n",
    "        for n in range(1, self.P+1):\n",
    "            # Standard SMC sampler, we mutate the particles and then we resample\n",
    "            ### Mutation step: \n",
    "            ###### Refresh velocities\n",
    "            z[:, self.d:] = np.random.normal(loc=0.0, scale=1.0, size=(self.N, self.d))\n",
    "            ###### Mutate positions \n",
    "            M = lambda z: THUG_MH(z, B, self.δ, self.ηs[n-1])\n",
    "            Z = np.apply_along_axis(M, 1, z)\n",
    "            ### Compute weights\n",
    "            w = (abs(np.apply_along_axis(f, 1, Z[:, :2]) - level_set_value) <= self.ϵs[n]).astype(float)\n",
    "            w = w / w.sum()\n",
    "            self.WEIGHTS[n] = w\n",
    "            ### Resample\n",
    "            indeces = choice(a=np.arange(self.N), size=self.N, p=w)\n",
    "            z = z[indeces, :]\n",
    "            self.PARTICLES[n] = z\n",
    "        self.total_time = time.time() - starting_time\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768c48a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MSTHUG_METROP = MarkovSnippetsTHUGMetropolised(N=N, B=B, δ=δ, d=2, ϵs=ϵs)\n",
    "zP_metrop = MSTHUG_METROP.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b046577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "for i in range(4):\n",
    "    ax[i].contour(*prep_contour([-2.5, 2.5], [-2.5, 2.5], 0.01, f), levels=[level_set_value])\n",
    "    ax[i].contour(*prep_contour([-2.5, 2.5], [-2.5, 2.5], 0.01, f), levels=[level_set_value-MSTHUG_METROP.ϵs[i], level_set_value+MSTHUG_METROP.ϵs[i]], colors='gray')\n",
    "    ax[i].scatter(*MSTHUG_METROP.PARTICLES[i, :, :2].T)\n",
    "    ax[i].set_xlim([-5, 5])\n",
    "    ax[i].set_ylim([-5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "for i in range(3):\n",
    "    _ = ax[i].hist(np.apply_along_axis(compute_arctan, 1, MSTHUG_METROP.PARTICLES[i, :, :2]), bins=30, density=True, weights=MSTHUG_METROP.WEIGHTS[i])\n",
    "    ax[i].set_xticks([-math.pi, 0, math.pi])\n",
    "    ax[i].set_xticklabels([r'$-\\mathregular{\\pi}$', r'$0$', r'$\\mathregular{\\pi}$'], fontsize=15)\n",
    "    ax[i].set_title(r'$\\mathregular{\\epsilon}=$' + '{}'.format(ϵs[i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c3a99",
   "metadata": {},
   "source": [
    "### Compare times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Markov Snippets Full Trajectory: {:.2}s\".format(MSTHUG.total_time))\n",
    "print(\"Markov Snippets End Only:        {:.2}s\".format(MSTHUG_ONLYEND.total_time))\n",
    "print(\"SMC-THUG (Metropolised version): {:.2}s\".format(MSTHUG_METROP.total_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4177be9b",
   "metadata": {},
   "source": [
    "### Plot histograms together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadeb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=3, figsize=(12, 9), sharex=True, sharey=True)\n",
    "# Markov Snippets - Full Trajectory (Version 1)\n",
    "for i in range(3):\n",
    "    _ = ax[0, i].hist(np.apply_along_axis(compute_arctan, 1, MSTHUG.ZNK[i, :, :2]), bins=30, density=True, weights=MSTHUG.Wbar[i])\n",
    "    ax[0, i].set_xticks([-math.pi, 0, math.pi])\n",
    "    ax[0, i].set_xticklabels([r'$-\\mathregular{\\pi}$', r'$0$', r'$\\mathregular{\\pi}$'], fontsize=15)\n",
    "    ax[0, i].set_title(r'$\\mathregular{\\epsilon}=$' + '{}'.format(ϵs[i+1]))\n",
    "# Markov Snippets - End-Only (Version 2)\n",
    "for i in range(3):\n",
    "    _ = ax[1, i].hist(np.apply_along_axis(compute_arctan, 1, MSTHUG_ONLYEND.ZNK[i, :, :2]), bins=30, density=True, weights=MSTHUG_ONLYEND.Wbar[i])\n",
    "    ax[1, i].set_xticks([-math.pi, 0, math.pi])\n",
    "    ax[1, i].set_xticklabels([r'$-\\mathregular{\\pi}$', r'$0$', r'$\\mathregular{\\pi}$'], fontsize=15)\n",
    "# SMC-THUG (Metropolised Version)\n",
    "for i in range(3):\n",
    "    _ = ax[2, i].hist(np.apply_along_axis(compute_arctan, 1, MSTHUG_METROP.PARTICLES[i, :, :2]), bins=30, density=True, weights=MSTHUG_METROP.WEIGHTS[i])\n",
    "    ax[2, i].set_xticks([-math.pi, 0, math.pi])\n",
    "    ax[2, i].set_xticklabels([r'$-\\mathregular{\\pi}$', r'$0$', r'$\\mathregular{\\pi}$'], fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(Number of particles with non-zero weight for ϵ_P) / total sampling time: \")\n",
    "print(\"MSv1: {:.1f}\".format(np.sum(MSTHUG.Wbar[-1] > 0) / MSTHUG.total_time))\n",
    "print(\"MSv2: {:.1f}\".format(np.sum(MSTHUG_ONLYEND.Wbar[-1] > 0) / MSTHUG_ONLYEND.total_time))\n",
    "print(\"SMC : {:.1f}\".format(np.sum(MSTHUG_METROP.WEIGHTS[-1] > 0) / MSTHUG_METROP.total_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc07c2",
   "metadata": {},
   "source": [
    "### Compute Means and other functionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d91601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea603a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bacef40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6059f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be0a4c67",
   "metadata": {},
   "source": [
    "- use $\\tilde{\\psi} = \\psi^T$ for $\\tilde{T} = 1$\n",
    "    - V1: keep initial and final point\n",
    "    - V2: keep only final point\n",
    "- Unfold ellipse $[0, 2\\pi]$ and show histogram for both algorithms, use $N(T+1)$ particles for histogram. \n",
    "- ESS for $N(T+1)$ particles, and ESS for $2N$ particles (SMC version - count $1$s)\n",
    "- Check functionals e.g. $x$, $x^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2320b",
   "metadata": {},
   "source": [
    "$$\\mathbb{I}(\\|f(x) - y\\| \\leq \\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215992ce",
   "metadata": {},
   "source": [
    "$$\n",
    "- \\epsilon \\leq f(x) - y \\leq \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2565d",
   "metadata": {},
   "source": [
    "$$\n",
    "f^{(i)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c929d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346f03e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
