{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d269318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import norm, solve\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fsolve\n",
    "from two_moons import tm_deterministic, tm_jacobian, tm_distance_gradient\n",
    "from numpy.random import uniform, normal, rand\n",
    "from scipy.stats import multivariate_normal as MVN\n",
    "from pandas import DataFrame as df\n",
    "from seaborn import pairplot\n",
    "from scipy.stats import uniform as udist\n",
    "from scipy.stats import norm as ndist\n",
    "from numpy import zeros, diag, eye, log, sqrt, vstack, mean, save, exp, linspace, pi\n",
    "from utils import ESS_univariate, ESS, n_unique\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from tangential_hug_functions import HugTangentialStepEJSD_Deterministic, Hop_Deterministic, HugStepEJSD_Deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce80a6",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d861585",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_star = np.zeros(2)\n",
    "T = 0.05                    # Integration time for Hug\n",
    "B = 5                       # Number of steps\n",
    "delta = T / B               # Step size\n",
    "N = 10000                   # Number of Hug iterations\n",
    "epsilon_spherical = 0.05\n",
    "epsilon_tangential = 0.05\n",
    "scale = 1.0                 # Scale for the MVN in the spherical case. Equiv to changing B and delta\n",
    "alpha = 0.6                 # Proportion of the gradient to remove from spherical to make tangential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a2593",
   "metadata": {},
   "source": [
    "# Initial Point on Manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc61b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample randomly the seeds\n",
    "a0 = uniform(low=-np.pi/2, high=np.pi/2)\n",
    "r0 = normal(loc=0.1, scale=0.01)\n",
    "\n",
    "# Start with a guess\n",
    "guess = np.array([0, 0, a0, r0])\n",
    "\n",
    "# Find point on manifold using optimization\n",
    "func = lambda xi: np.r_[tm_deterministic(xi) - y_star, 0, 0]  # Append 0, 0 to make fsolve work.\n",
    "xi0 = fsolve(func, guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a82a0",
   "metadata": {},
   "source": [
    "# Functions for HUG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c65522",
   "metadata": {},
   "source": [
    "$$\n",
    "u = (a, r) \\qquad \\text{with} \\qquad p(a) = \\mathcal{U}\\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right)\\qquad \\text{and}\\qquad p(r) = \\mathcal{N}(0.1, 0.01^2) \\qquad \\text{hence} \\qquad p(u) = \\mathcal{U}\\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right)\\mathcal{N}(0.1, 0.01^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac37b7",
   "metadata": {},
   "source": [
    "$$\n",
    "p(\\theta) = \\mathcal{U}\\left([-1, 1]\\times [-1, 1]\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1467b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprior(xi):\n",
    "    \"\"\"Log prior distribution for Two Moon.\"\"\"\n",
    "    t1, t2, a, r = xi\n",
    "    return udist(loc=-1, scale=2).logpdf(t1) + udist(loc=-1, scale=2).logpdf(t2) + udist(loc=-np.pi/2, scale=np.pi).logpdf(a) + ndist(0.1, 0.01).logpdf(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3e4efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_function = lambda xi: tm_distance_gradient(xi, y_star, epsilon).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ba3148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_uniform_kernel(xi, epsilon):\n",
    "    \"\"\"Log density of uniform kernel. \"\"\"\n",
    "    with np.errstate(divide='ignore'):\n",
    "        return np.log((norm(tm_deterministic(xi) - y_star) <= epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4c3c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_abc_posterior(xi):\n",
    "    \"\"\"Log density of ABC posterior. Product of (param-latent) prior and uniform kernel.\"\"\"\n",
    "    return logprior(xi) + log_uniform_kernel(xi, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c67698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(x00, T1, T2, N, alphas, nlags):\n",
    "    \"\"\"Runs Hug+Hop and THUG+HOP using the same velocities and the same random seeds.\"\"\"\n",
    "    ### COMMON VARIABLES\n",
    "    v = q.rvs(N)\n",
    "    log_uniforms1 = log(rand(N))     # Log uniforms for the HUG kernels\n",
    "    log_uniforms2 = log(rand(N))     # Log uniforms for the HOP kernel\n",
    "    u = MVN(zeros(4), eye(4)).rvs(N) # Original velocities for HOP kernel\n",
    "    ### STORAGE (HUG + HOP)\n",
    "    hh = x00              # Initial sample\n",
    "    ahh1 = 0.0            # Acceptance probability for HUG kernel\n",
    "    ahh2 = 0.0            # Acceptance probability for HOP kernel (when used with HUG)\n",
    "    ehh = 0.0             # EJSD\n",
    "    eghh = 0.0            # EJSD in Gradient direction\n",
    "    ethh = 0.0            # EJSD in Tangent direction\n",
    "    ### STORAGE (THUG + HOP) I MUST STORE FOR ALL ALPHAS\n",
    "    ath1 = zeros(n_alphas)\n",
    "    ath2 = zeros(n_alphas)\n",
    "    eth  = zeros(n_alphas)\n",
    "    egth = zeros(n_alphas)\n",
    "    etth = zeros(n_alphas)\n",
    "    ### ADDITIONAL STORAGE FOR THUG\n",
    "    th_esst = zeros(n_alphas)\n",
    "    th_essu = zeros(n_alphas)\n",
    "    th_essj = zeros(n_alphas)\n",
    "    th_uniq = zeros(n_alphas)\n",
    "    th_act  = zeros((n_alphas, nlags))\n",
    "    th_acu  = zeros((n_alphas, nlags))\n",
    "    ### HUG + HOP\n",
    "    x = x00\n",
    "    for i in range(N):\n",
    "        y, a1, e, eg, et = HugStepEJSD_Deterministic(x, v[i], log_uniforms1[i], T1, B, q, log_abc_posterior, grad_function)\n",
    "        x, a2 = Hop_Deterministic(y, u[i], log_uniforms2[i], lam, kappa, log_abc_posterior, grad_function)\n",
    "        hh = vstack((hh, y, x))\n",
    "        ahh1 += a1 * 100 / N\n",
    "        ahh2 += a2 * 100 / N\n",
    "        ehh += e / N\n",
    "        eghh += eg / N \n",
    "        ethh += et / N \n",
    "    # COMPUTE ESS AND OTHER METRICS FOR HUG\n",
    "    hh = hh[1:]\n",
    "    hh_esst = ESS_univariate(hh[::2, 0])     # ESS for theta\n",
    "    hh_essu = ESS_univariate(hh[::2, 1])     # ESS for u\n",
    "    hh_essj = ESS(hh[::2])                   # ESS joint\n",
    "    hh_uniq = n_unique(hh)                             # Number of unique samples\n",
    "    hh_act  = acf(hh[::2, 0], adjusted=True, nlags=nlags, fft=True)[1:]  # Autocorrelation for theta (remove the first 1.0)\n",
    "    hh_acu  = acf(hh[::2, 1], adjusted=True, nlags=nlags, fft=True)[1:]  # Autocorrelation for u\n",
    "    ### THUG + HOP\n",
    "    for k, alpha in enumerate(alphas):\n",
    "        x = x00\n",
    "        th = x00      # RESTART THE SAMPLES FROM SCRATCH\n",
    "        for i in range(N):\n",
    "            y, a1, e, eg, et = HugTangentialStepEJSD_Deterministic(x, v[i], log_uniforms1[i], T2, B, alpha, q, log_abc_posterior, grad_function)\n",
    "            x, a2 = Hop_Deterministic(y, u[i], log_uniforms2[i], lam, kappa, log_abc_posterior, grad_function)\n",
    "            th = vstack((th, y, x))\n",
    "            ath1[k] += a1 * 100 / N\n",
    "            ath2[k] += a2 * 100 / N\n",
    "            eth[k]  += e / N\n",
    "            egth[k] += eg / N \n",
    "            etth[k] += et / N \n",
    "        ### COMPUTE ESS AND OTHER METRISC FOR THUG\n",
    "        th = th[1:]\n",
    "        th_esst[k] = ESS_univariate(th[::2, 0])     # ESS for theta\n",
    "        th_essu[k] = ESS_univariate(th[::2, 1])     # ESS for u\n",
    "        th_essj[k] = ESS(th[::2])                   # ESS joint\n",
    "        th_uniq[k] = n_unique(th)                             # Number of unique samples\n",
    "        th_act[k] = acf(th[::2, 0], adjusted=True, nlags=nlags, fft=True)[1:]  # Autocorrelation for theta\n",
    "        th_acu[k] = acf(th[::2, 1], adjusted=True, nlags=nlags, fft=True)[1:]  # Autocorrelation for u\n",
    "    # RETURN EVERYTHING\n",
    "    out = {\n",
    "        'HH': {\n",
    "            'A1': ahh1,\n",
    "            'A2': ahh2,\n",
    "            'E': ehh,\n",
    "            'EG': eghh, \n",
    "            'ET': ethh,\n",
    "            'ESS_T': hh_esst,\n",
    "            'ESS_U': hh_essu,\n",
    "            'ESS_J': hh_essj,\n",
    "            'UNIQUE': hh_uniq,\n",
    "            'AC_T': hh_act,\n",
    "            'AC_U': hh_acu,\n",
    "            'T': T1,\n",
    "            'SAMPLES': hh\n",
    "        },\n",
    "        'TH': {\n",
    "            'A1': ath1,\n",
    "            'A2': ath2,\n",
    "            'E': eth,\n",
    "            'EG': egth, \n",
    "            'ET': etth, \n",
    "            'ESS_T': th_esst,\n",
    "            'ESS_U': th_essu,\n",
    "            'ESS_J': th_essj,\n",
    "            'UNIQUE': th_uniq,\n",
    "            'AC_T': th_act,\n",
    "            'AC_U': th_acu,\n",
    "            'T': T2,\n",
    "            'SAMPLES': th\n",
    "        }\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "937a28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = MVN(zeros(4), eye(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a95511a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.9]\n",
    "n_alphas = len(alphas)\n",
    "epsilon = 0.05\n",
    "lam = epsilon / 15 \n",
    "kappa = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = experiment(xi0, 0.05, 0.05, 10000, alphas, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bd31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8), ncols=2)\n",
    "ax[0].scatter(*out['HH']['SAMPLES'][:, :2].T)\n",
    "ax[0].set_aspect(\"equal\")\n",
    "ax[1].scatter(*out['TH']['SAMPLES'][:, :2].T)\n",
    "ax[1].set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743234ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "out['HH']['A1'], *out['TH']['A1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bfb7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out['HH']['UNIQUE'], *out['TH']['UNIQUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out['HH']['A2'], *out['TH']['A2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc39752",
   "metadata": {},
   "outputs": [],
   "source": [
    "!say FINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a2fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
